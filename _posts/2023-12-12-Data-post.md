---
layout: post
title:  "How to Use R to Analyze and Visualize College Basketball Data"
author: Matt Lindeman
description: A post about college basketball data that I collected and cleaned
image: "/assets/images/markus-spiske-BfphcCvhl6E-unsplash.jpg"
---

College basketball is a part of my daily life, so choosing to analyze some surface level analytics data came as an easy choice. I decided to begin this process by scraping data that could help me take a look at what statistics factor into regular season wins, and how some teams and conferencers differ. This post will proceed by discussing how the data was collected and cleaned, followed by the considerations I took while web scraping, and finishing with a few concluding thoughts.

# Python Packages

For data collection I used BeautifulSoup, along with some other packages, to scrape and clean.

```
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
```

# Barttorvik

I used the Barttorvik website to collect data from. It's a basketball analytics website that calculates more advanced statistics from data such as box scores and play-by-play data. I used the following code to scrape the data and format the table into a dataframe.

```
# 2023 Data
url = 'https://barttorvik.com/trank.php?year=2023&sort=&top=0&conlimit=All#'
response = requests.get(url)

soup = BeautifulSoup(response.content, 'html.parser')
table = soup.find('table')

# Process the table data
if table:
    rows = table.find_all('tr')

    data = []
    headers = []
    for row in rows:
        columns = row.find_all(['th', 'td'])
        row_data = []
        for i, col in enumerate(columns):
            row_data.append(col.get_text())
            if len(headers) < len(columns):  # Handle missing headers
                headers.append(f'Column {i + 1}')
        data.append(row_data)

    # Convert data into a DataFrame
    df_2023 = pd.DataFrame(data)
```
I grabbed the data for 2019-2023 seasons, using the same code as above with slight alterations to get all 5 years.

# Data Cleaning

The biggest hurdle I faced when cleaning the data was dealing with the extra numbers and headings that came with how the table is formatted on the webpage. The table shows a lot of rankings for each statistic and those rankings tend to get lumped with the number when scraping, so I had to remove the excess numbers.

```
# Fixing the Barthag column
df_bart['Barthag'] = df_bart['Barthag'].apply(lambda x: re.search(r'\.\d{4}', str(x)).group(0) if re.search(r'\.\d{4}', str(x)) else x)
```

Along with that, the webpage had headers every 50 rows that needed to be removed from the data.

```
# Getting rid of rows that have the column names
first_two_columns = df_2023.columns[:2]
df_2023 = df_2023[~(df_2023[first_two_columns].astype(str) == df_2023.iloc[:, :2].columns).all(axis=1)]
```

Beyond that the cleaning was minor. A added a column that signifies which season that team data was from. I split the record column which had a format like 30-7 into two columns, one for wins and one for losses. I also dropped a couple columns that had numbers beyond the scope of my analysis.

# Additional Data

After creating that dataset containing team data from 2019-2023 I wanted to add a few more numbers that I felt would help my analysis. I went to a different area on the Barttorvik website to get strength of schedule data. A strength of schedule rating is a number, in this case something from 0 to 1, that indicates how difficult a team's schedule was based on the skill level of the teams they faced. I felt this metric would help my analysis epsecially when it came to analyzing a team's wins. Some teams have a 30 win season, but never play very difficult opponents, and some teams have a 30 win season by play almost exclusively top rated opponents. With that in mind, understanding strength of schedule will help me weight, to a degree, a team's wins.

I gathered and cleaned the data in a similar fashion as earlier, using beautiful soup and the following code. 

```
# 2023 Strength of Schedule Data
url = 'https://barttorvik.com/sos.php?year=2023&conlimit=&sort=10'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

response = requests.get(url, headers=headers)

if response.status_code == 200:
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Find the table on the webpage
    table = soup.find('table')

    # Extract table data, excluding the first row (assumed to be headers)
    table_data = []
    for row in table.find_all('tr')[1:]:
        row_data = [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])][:9]  # Exclude columns beyond index 9
        if row_data:
            table_data.append(row_data)

    # Convert table data to DataFrame
    df_23 = pd.DataFrame(table_data)
```

Cleaning this data was also very similar to above so I went through the same processes. I removed the additional header rows, I cleaned a few of the numbers to remove the ranking numbers, and I removed a few columns that were specifically about only non-conference games.

I then merged the dataframes and saved them to a csv file.

```
# Merging the two dataframes
merged_df = pd.merge(df_bart, df_sos, on=['Team', 'Conf', 'Season'])

# Saving the dataframe to a CSV file
merged_df.to_csv('CBBdata19-23.csv', index=False)
```

# Ethical Considerations
The data and numbers used on the Barttorvik website are all free to the public. Most of the data is available through csv links, however the tables I wanted to use didn't have csv links. The site does allow small scale scraping to take place, so I made sure not to exceed rate limits that would harm the websites performance.

# Conclusion

I'm glad to have had this opportunity to share my data collection and cleaning process with you. I hope to expand into more webpages about college basketball, eventually scraping data from websites like ESPN and NCAA. I will be sharing a new blog post soon regarding my EDA process for this data and I encourage you to check it out!

For my full code, visit this repo: [STAT 386 Project](https://github.com/MattLindeman/STAT386-Project)
